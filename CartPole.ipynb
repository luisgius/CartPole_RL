{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set randoms seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enviroment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        # Simple NN with one hidden layer\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,action_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Convert State to tensor\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.FloatTensor(x).unsqueeze(0)\n",
    "\n",
    "        # Forward pass through the network to get action probabilities\n",
    "        action_probs = self.network(x)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize Policy Network\n",
    "state_dim = env.observation_space.shape[0]  # 4 for CartPole (position, velocity, angle, angular velocity)\n",
    "action_dim = env.action_space.n # 2 for CartPole (push left, push right)\n",
    "policy_network = PolicyNetwork(state_dim, action_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect a single episode\n",
    "def collect_episode(policy_network, env):\n",
    "\n",
    "    # Initialize lists to store the episode data\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    # Reset the enviroment\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # Collect episode data\n",
    "    while not done:\n",
    "\n",
    "        # Store current state\n",
    "        states.append(state)\n",
    "\n",
    "        # Get action probabilities\n",
    "        action_probs = policy_network(state)\n",
    "\n",
    "        # Sample action from the policy\n",
    "        action_dist = Categorical(action_probs)\n",
    "        action = action_dist.sample()\n",
    "\n",
    "        # Store action\n",
    "        actions.append(action)\n",
    "\n",
    "        # Take the action in the enviroment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Store reward\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "    return states, actions, rewards\n",
    "\n",
    "\n",
    "# Function to compute returns (Discounted Future Rewards)\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "    G = 0\n",
    "\n",
    "    # Iterate over the rewards in reverse order\n",
    "    for reward in reversed(rewards):\n",
    "        G = reward + gamma * G\n",
    "        returns.insert(0, G) # Insert at the beginning\n",
    "\n",
    "    # Convert to tensor and normalize\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-9) # Normalize returns for stable learning\n",
    "\n",
    "    return returns\n",
    "\n",
    "# Main Training Loop\n",
    "def train_reinforce(policy_network, env, num_episodes=1000):\n",
    "\n",
    "    # List to store metrics\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        # Collect episode data\n",
    "        states, actions, rewards = collect_episode(policy_network, env)\n",
    "\n",
    "        # Compute returns\n",
    "        returns = compute_returns(rewards)\n",
    "\n",
    "        # Compute loss\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "\n",
    "         # Compute log probabilities for each action taken\n",
    "        for t in range(len(states)):\n",
    "            state = states[t]\n",
    "            action = actions[t]\n",
    "            G = returns[t]\n",
    "\n",
    "            #Get action probabilities\n",
    "            action_probs = policy_network(state)\n",
    "\n",
    "            # Create a distribution and compute log probability of the taken action\n",
    "            action_dist = Categorical(action_probs)\n",
    "            log_prob = action_dist.log_prob(torch.tensor(action))\n",
    "\n",
    "            # Add negative log probability multiplied by return to loss\n",
    "            # (negative because we're minimizing loss but want to maximize expected return)\n",
    "            loss += -log_prob * G\n",
    "\n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the policy network\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record total reward for the episode\n",
    "        total_reward = sum(rewards)\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # Print progress\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            print(f\"Episode {episode+1}/{num_episodes}, Average Reward (last 10): {avg_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis\\AppData\\Local\\Temp\\ipykernel_13308\\4237544408.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  log_prob = action_dist.log_prob(torch.tensor(action))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10/1000, Average Reward (last 10): 22.40\n",
      "Episode 20/1000, Average Reward (last 10): 28.80\n",
      "Episode 30/1000, Average Reward (last 10): 32.50\n",
      "Episode 40/1000, Average Reward (last 10): 103.10\n",
      "Episode 50/1000, Average Reward (last 10): 241.00\n",
      "Episode 60/1000, Average Reward (last 10): 64.60\n",
      "Episode 70/1000, Average Reward (last 10): 28.40\n",
      "Episode 80/1000, Average Reward (last 10): 39.60\n",
      "Episode 90/1000, Average Reward (last 10): 128.70\n",
      "Episode 100/1000, Average Reward (last 10): 384.50\n",
      "Episode 110/1000, Average Reward (last 10): 500.00\n",
      "Episode 120/1000, Average Reward (last 10): 455.80\n",
      "Episode 130/1000, Average Reward (last 10): 429.00\n",
      "Episode 140/1000, Average Reward (last 10): 480.20\n",
      "Episode 150/1000, Average Reward (last 10): 491.40\n",
      "Episode 160/1000, Average Reward (last 10): 467.60\n",
      "Episode 170/1000, Average Reward (last 10): 479.20\n",
      "Episode 180/1000, Average Reward (last 10): 477.00\n",
      "Episode 190/1000, Average Reward (last 10): 489.90\n",
      "Episode 200/1000, Average Reward (last 10): 500.00\n",
      "Episode 210/1000, Average Reward (last 10): 178.90\n",
      "Episode 220/1000, Average Reward (last 10): 449.60\n",
      "Episode 230/1000, Average Reward (last 10): 500.00\n",
      "Episode 240/1000, Average Reward (last 10): 500.00\n",
      "Episode 250/1000, Average Reward (last 10): 453.10\n",
      "Episode 260/1000, Average Reward (last 10): 153.70\n",
      "Episode 270/1000, Average Reward (last 10): 177.50\n",
      "Episode 280/1000, Average Reward (last 10): 115.60\n",
      "Episode 290/1000, Average Reward (last 10): 107.80\n",
      "Episode 300/1000, Average Reward (last 10): 108.60\n",
      "Episode 310/1000, Average Reward (last 10): 103.80\n",
      "Episode 320/1000, Average Reward (last 10): 150.40\n",
      "Episode 330/1000, Average Reward (last 10): 165.50\n",
      "Episode 340/1000, Average Reward (last 10): 181.30\n",
      "Episode 350/1000, Average Reward (last 10): 400.70\n",
      "Episode 360/1000, Average Reward (last 10): 315.40\n",
      "Episode 370/1000, Average Reward (last 10): 500.00\n",
      "Episode 380/1000, Average Reward (last 10): 500.00\n",
      "Episode 390/1000, Average Reward (last 10): 500.00\n",
      "Episode 400/1000, Average Reward (last 10): 500.00\n",
      "Episode 410/1000, Average Reward (last 10): 500.00\n",
      "Episode 420/1000, Average Reward (last 10): 500.00\n",
      "Episode 430/1000, Average Reward (last 10): 500.00\n",
      "Episode 440/1000, Average Reward (last 10): 500.00\n",
      "Episode 450/1000, Average Reward (last 10): 416.40\n",
      "Episode 460/1000, Average Reward (last 10): 43.40\n",
      "Episode 470/1000, Average Reward (last 10): 15.50\n",
      "Episode 480/1000, Average Reward (last 10): 29.90\n",
      "Episode 490/1000, Average Reward (last 10): 106.60\n",
      "Episode 500/1000, Average Reward (last 10): 72.80\n",
      "Episode 510/1000, Average Reward (last 10): 32.80\n",
      "Episode 520/1000, Average Reward (last 10): 46.40\n",
      "Episode 530/1000, Average Reward (last 10): 69.00\n",
      "Episode 540/1000, Average Reward (last 10): 93.50\n",
      "Episode 550/1000, Average Reward (last 10): 86.60\n",
      "Episode 560/1000, Average Reward (last 10): 41.60\n",
      "Episode 570/1000, Average Reward (last 10): 21.30\n",
      "Episode 580/1000, Average Reward (last 10): 35.30\n",
      "Episode 590/1000, Average Reward (last 10): 57.20\n",
      "Episode 600/1000, Average Reward (last 10): 70.40\n",
      "Episode 610/1000, Average Reward (last 10): 84.10\n",
      "Episode 620/1000, Average Reward (last 10): 116.10\n",
      "Episode 630/1000, Average Reward (last 10): 122.80\n",
      "Episode 640/1000, Average Reward (last 10): 128.20\n",
      "Episode 650/1000, Average Reward (last 10): 188.10\n",
      "Episode 660/1000, Average Reward (last 10): 246.50\n",
      "Episode 670/1000, Average Reward (last 10): 192.50\n",
      "Episode 680/1000, Average Reward (last 10): 186.80\n",
      "Episode 690/1000, Average Reward (last 10): 274.70\n",
      "Episode 700/1000, Average Reward (last 10): 385.70\n",
      "Episode 710/1000, Average Reward (last 10): 332.90\n",
      "Episode 720/1000, Average Reward (last 10): 416.60\n",
      "Episode 730/1000, Average Reward (last 10): 500.00\n",
      "Episode 740/1000, Average Reward (last 10): 500.00\n",
      "Episode 750/1000, Average Reward (last 10): 500.00\n",
      "Episode 760/1000, Average Reward (last 10): 500.00\n",
      "Episode 770/1000, Average Reward (last 10): 500.00\n",
      "Episode 780/1000, Average Reward (last 10): 500.00\n",
      "Episode 790/1000, Average Reward (last 10): 500.00\n",
      "Episode 800/1000, Average Reward (last 10): 500.00\n",
      "Episode 810/1000, Average Reward (last 10): 500.00\n",
      "Episode 820/1000, Average Reward (last 10): 500.00\n",
      "Episode 830/1000, Average Reward (last 10): 500.00\n",
      "Episode 840/1000, Average Reward (last 10): 500.00\n",
      "Episode 850/1000, Average Reward (last 10): 500.00\n",
      "Episode 860/1000, Average Reward (last 10): 500.00\n",
      "Episode 870/1000, Average Reward (last 10): 500.00\n",
      "Episode 880/1000, Average Reward (last 10): 500.00\n",
      "Episode 890/1000, Average Reward (last 10): 500.00\n",
      "Episode 900/1000, Average Reward (last 10): 500.00\n",
      "Episode 910/1000, Average Reward (last 10): 500.00\n",
      "Episode 920/1000, Average Reward (last 10): 500.00\n",
      "Episode 930/1000, Average Reward (last 10): 500.00\n",
      "Episode 940/1000, Average Reward (last 10): 500.00\n",
      "Episode 950/1000, Average Reward (last 10): 500.00\n",
      "Episode 960/1000, Average Reward (last 10): 500.00\n",
      "Episode 970/1000, Average Reward (last 10): 500.00\n",
      "Episode 980/1000, Average Reward (last 10): 500.00\n",
      "Episode 990/1000, Average Reward (last 10): 500.00\n",
      "Episode 1000/1000, Average Reward (last 10): 500.00\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "reward_history = train_reinforce(policy_network, env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 500.0, Steps = 500\n",
      "Episode 2: Reward = 500.0, Steps = 500\n",
      "Episode 3: Reward = 500.0, Steps = 500\n",
      "Episode 4: Reward = 500.0, Steps = 500\n",
      "Episode 5: Reward = 500.0, Steps = 500\n",
      "\n",
      "Evaluation Results:\n",
      "Average Reward: 500.00\n",
      "Average Steps: 500.00\n",
      "Best Episode Reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "def visualize_policy_simple(policy_network, env_name='CartPole-v1', episodes=3):\n",
    "    \"\"\"\n",
    "    A simplified function to evaluate and report on a trained policy network.\n",
    "    This version avoids heavy rendering that might crash the kernel.\n",
    "    \"\"\"\n",
    "    # Create environment without rendering\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Get action probabilities from policy network\n",
    "            action_probs = policy_network(state)\n",
    "            \n",
    "            # Select the action with highest probability\n",
    "            action = torch.argmax(action_probs).item()\n",
    "            \n",
    "            # Take the action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_steps.append(steps)\n",
    "        print(f\"Episode {episode+1}: Reward = {total_reward}, Steps = {steps}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "    avg_steps = sum(episode_steps) / len(episode_steps)\n",
    "    \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "    print(f\"Average Steps: {avg_steps:.2f}\")\n",
    "    print(f\"Best Episode Reward: {max(episode_rewards)}\")\n",
    "    \n",
    "    # Create a simple bar chart of episode rewards\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, episodes+1), episode_rewards, color='blue')\n",
    "    plt.axhline(y=avg_reward, color='r', linestyle='--', label=f'Average: {avg_reward:.2f}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Performance of Trained Policy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# After training your policy network\n",
    "results = visualize_policy_simple(policy_network, episodes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
